{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from dataclasses import dataclass\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nimport math\nimport time\ntorch.set_float32_matmul_precision('high')","metadata":{"_uuid":"af8a95c2-8830-4a42-85b4-cd877b2f9a2a","_cell_guid":"e9f9a2c7-6895-4fc1-99fd-397263001556","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-06-28T18:12:08.193350Z","iopub.execute_input":"2025-06-28T18:12:08.194121Z","iopub.status.idle":"2025-06-28T18:12:12.598193Z","shell.execute_reply.started":"2025-06-28T18:12:08.194094Z","shell.execute_reply":"2025-06-28T18:12:12.597393Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"from torch.distributed import init_process_group, destroy_process_group\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nimport torch.distributed as dist\nimport os","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T18:12:12.598899Z","iopub.execute_input":"2025-06-28T18:12:12.599286Z","iopub.status.idle":"2025-06-28T18:12:12.604110Z","shell.execute_reply.started":"2025-06-28T18:12:12.599266Z","shell.execute_reply":"2025-06-28T18:12:12.603246Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"ddp = int(os.environ.get('RANK', -1)) != -1 # is this a ddp run?\nif ddp:\n    # use of DDP atm demands CUDA, we set the device appropriately according to rank\n    print(\"ddp\")\n    assert torch.cuda.is_available(), \"for now i think we need CUDA for DDP\"\n    init_process_group(backend='nccl')\n    ddp_rank = int(os.environ['RANK'])\n    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n    ddp_world_size = int(os.environ['WORLD_SIZE'])\n    device = f'cuda:{ddp_local_rank}'\n    torch.cuda.set_device(device)\n    master_process = ddp_rank == 0 # this process will do logging, checkpointing etcls\nelse:\n    # vanilla, non-DDP run\n    ddp_rank = 0\n    ddp_local_rank = 0\n    ddp_world_size = 1\n    master_process = True\n    # attempt to autodetect device\n    device = \"cpu\"\n    if torch.cuda.is_available():\n        device = \"cuda\"\n    elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n        device = \"mps\"\n    print(f\"using device: {device}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T18:12:12.606808Z","iopub.execute_input":"2025-06-28T18:12:12.607025Z","iopub.status.idle":"2025-06-28T18:12:12.693801Z","shell.execute_reply.started":"2025-06-28T18:12:12.607009Z","shell.execute_reply":"2025-06-28T18:12:12.693072Z"}},"outputs":[{"name":"stdout","text":"using device: cuda\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"@dataclass\nclass GPTConfig:\n    block_size: int = 1024 # context windows\n    vocab_size: int = 50257 # unique tokens\n    n_layer: int = 12 # number of blocks\n    n_head: int = 12 # n head inside a casual self attention\n    n_embd: int = 768 # size of the embedding vector array","metadata":{"_uuid":"06e67817-2138-44ea-bec0-b269d4547eb7","_cell_guid":"d02e3bee-5b77-44ab-8b64-5a8fe1e3926d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-06-28T18:12:12.694797Z","iopub.execute_input":"2025-06-28T18:12:12.695096Z","iopub.status.idle":"2025-06-28T18:12:12.711256Z","shell.execute_reply.started":"2025-06-28T18:12:12.695072Z","shell.execute_reply":"2025-06-28T18:12:12.710556Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class MLP(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.c_fc = nn.Linear(config.n_embd, config.n_embd * 4)\n        self.gelu = nn.GELU(approximate='tanh')\n        self.c_proj = nn.Linear(config.n_embd * 4, config.n_embd)\n\n    \n        self.c_proj.NANOGPT_SCALE_INIT = 1\n    \n    def forward(self, x):\n        # print(\"Inside MLP\")\n        x = self.c_fc(x)\n        x = self.gelu(x)\n        x = self.c_proj(x)\n        return x","metadata":{"_uuid":"0bf79727-1b99-4501-9298-39f7227f7ba8","_cell_guid":"fa59eddd-440f-4774-b7ca-a3d3257262fa","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-06-28T18:12:12.712098Z","iopub.execute_input":"2025-06-28T18:12:12.712616Z","iopub.status.idle":"2025-06-28T18:12:12.731054Z","shell.execute_reply.started":"2025-06-28T18:12:12.712573Z","shell.execute_reply":"2025-06-28T18:12:12.730138Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class CausalSelfAttention(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n        self.n_head = config.n_head\n        self.n_emdb = config.n_embd\n\n        self.c_proj.NANOGPT_SCALE_INIT = 1\n        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n                            .view(1, 1, config.block_size, config.block_size))\n\n    def forward(self, x):\n        # print(\"Inside self attention\")\n        B, T, C = x.size()\n\n        qkv = self.c_attn(x)\n        q, k, v = qkv.split(self.n_emdb, dim=2)\n        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n\n        att = (q @ k.transpose(-2, -1)) * (1.0/math.sqrt(k.size()[-1]))\n        att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n        att = F.softmax(att, dim=-1)\n        y = att @ v\n\n        # y = F.scaled_dot_product_attention(q, k, v, is_causal=True)        \n        y = y.transpose(1, 2).contiguous().view(B, T, C)\n        y = self.c_proj(y)\n        return y","metadata":{"_uuid":"6423d2a7-3a4e-44b9-96ef-c5b10d716ad6","_cell_guid":"4b2ffa55-d112-478d-b4dd-46bd831d698e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-06-28T18:12:12.731946Z","iopub.execute_input":"2025-06-28T18:12:12.732326Z","iopub.status.idle":"2025-06-28T18:12:12.751435Z","shell.execute_reply.started":"2025-06-28T18:12:12.732300Z","shell.execute_reply":"2025-06-28T18:12:12.750767Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class Block(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.ln_1 = nn.LayerNorm(config.n_embd)\n        self.attn = CausalSelfAttention(config)\n        self.ln_2 = nn.LayerNorm(config.n_embd)\n        self.mlp = MLP(config)\n\n    def forward(self, x):\n        # print(\"Inside block \")\n        x = x + self.attn(self.ln_1(x))\n        x = x + self.mlp(self.ln_2(x))\n\n        return x","metadata":{"_uuid":"48de396a-b607-4c1a-95a0-a69ff34b3c46","_cell_guid":"86ee1e3c-17e1-40ae-ae15-923131194cf4","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-06-28T18:12:12.752199Z","iopub.execute_input":"2025-06-28T18:12:12.752533Z","iopub.status.idle":"2025-06-28T18:12:12.768047Z","shell.execute_reply.started":"2025-06-28T18:12:12.752515Z","shell.execute_reply":"2025-06-28T18:12:12.767285Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"class GPT(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n\n        self.config = config\n        self.transformer = nn.ModuleDict(dict(\n            wte = nn.Embedding(config.vocab_size, config.n_embd),\n            wpe = nn.Embedding(config.block_size, config.n_embd),\n            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n            ln_f = nn.LayerNorm(config.n_embd)\n        ))\n        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n\n        # if torch.cuda.device_count() > 1:\n        #     print(f\"Using {torch.cuda.device_count()} GPUs\")\n        #     self = nn.DataParallel(self)\n\n        #shared weights\n        self.transformer.wte.weight = self.lm_head.weight\n\n        self.apply(self._init_weight)\n    \n    def _init_weight(self, module):\n        if isinstance(module, nn.Linear):\n            std = 0.02\n            if hasattr(module, 'NANOGPT_SCALE_INIT'):\n                std *= (2 * self.config.n_layer) ** -0.5\n            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n            if module.bias is not None:\n                torch.nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n        \n    def configure_optimizer(self, weight_decay, lr, device):\n        import inspect\n        param_dict = {pn: p for pn, p in self.named_parameters()}\n        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n\n        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n\n        optim_group = [\n            {'params': decay_params, 'weight_decay': weight_decay},\n            {'params': nodecay_params, 'weight_decay': 0.0},\n        ]\n\n        num_decay_params = sum(p.numel() for p in decay_params)\n        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n\n        print(f\"Num decayed paramter tensor: {len(decay_params)}, with {num_decay_params:,} params\")\n        \n        print(f\"Num non decayed paramter tensor: {len(nodecay_params)}, with {num_nodecay_params:,} params\")\n\n        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n        used_fused = fused_available and 'cuda' in device\n        print(f\"using fused AdamW: {used_fused}\")\n        return torch.optim.AdamW(optim_group, lr=lr, betas=(0.9, 0.95), eps=1e-8)\n    \n    def forward(self, idx, target=None):\n        module = self##.module if isinstance(self, nn.DataParallel) else self\n        B, T = idx.size()\n        # assert T <= module.config.block_size, f\"Cannot forward sequence length {T}, block size: {module.config.block_size}\"\n\n        device = idx.device\n\n        pos = torch.arange(0, T, dtype=torch.long, device=device)\n        pos_embd = module.transformer.wpe(pos)\n        tok_embd = module.transformer.wte(idx)\n        x = tok_embd + pos_embd\n\n        for block in module.transformer.h:\n            x = block(x)\n\n        x = module.transformer.ln_f(x)\n        logits = module.lm_head(x)\n\n        B, T, vocab_size = logits.size()\n        loss = None\n        if target is not None:\n        \n            loss = F.cross_entropy(logits.view(-1, vocab_size), target.view(-1))\n\n        return logits, loss\n\n\n    @classmethod\n    def from_pretrained(cls, model_type):\n        \"\"\"Load pretrained GPT-2 from hugging face\"\"\"\n\n        print(\"test\")\n        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n        from transformers import GPT2LMHeadModel\n        print(\"loading weights from pretrained gpt: %s\" % model_type)\n\n        config_args = {\n            'gpt2':           dict(n_layer=12, n_head=12, n_embd=768),\n            'gpt2-medium':    dict(n_layer=24, n_head=16, n_embd=1024),\n            'gpt2-large':      dict(n_layer=36, n_head=20, n_embd=1280),\n            'gpt2-xl':         dict(n_layer=48, n_head=25, n_embd=1600),\n        }[model_type]\n\n        config_args['vocab_size'] = 50257\n        config_args['block_size'] = 1024\n\n        config = GPTConfig(**config_args)\n        model = GPT(config)\n\n        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n        sd_hf = model_hf.state_dict()\n\n        if isinstance(model, nn.DataParallel):\n            sd_hf_modified = {f'module.{k}': v for k, v in sd_hf.items()}\n            sd_hf = sd_hf_modified\n\n        sd = model.state_dict()\n\n        sd_keys = sd.keys()\n        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')]\n\n        sd_keys_hf = sd_hf.keys()\n        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')]\n        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')]\n\n        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n\n        assert len(sd_keys_hf) == len(sd_keys), f\"missmatched keys: {len(sd_keys_hf) != len(sd_keys)}\"\n\n        for k in sd_keys_hf:\n            if any(k.endswith(w) for w in transposed):\n                 # Adjust the key for the transposed check if DataParallel is used\n                k_orig = k.replace('module.', '') if isinstance(model, nn.DataParallel) else k\n                assert sd_hf[k].shape[::-1] == sd[k].shape, f\"Shape mismatch on transposed {k_orig}\"\n                with torch.no_grad():\n                    sd[k].copy_(sd_hf[k].t())\n            else:\n                assert sd_hf[k].shape == sd[k].shape, f\"Shape mismatch on {k}\"\n                with torch.no_grad():\n                    sd[k].copy_(sd_hf[k])\n\n\n        return model","metadata":{"_uuid":"8edb21ee-cfa6-4f14-8d35-404c8f1d42d2","_cell_guid":"aa9027f9-5ae7-47b6-a466-2fca0e734d0a","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-06-28T18:12:12.768909Z","iopub.execute_input":"2025-06-28T18:12:12.769173Z","iopub.status.idle":"2025-06-28T18:12:12.791469Z","shell.execute_reply.started":"2025-06-28T18:12:12.769147Z","shell.execute_reply":"2025-06-28T18:12:12.790615Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"torch.manual_seed(42)\nif torch.cuda.is_available():\n    torch.manual_seed(1337)\n# model = GPT.from_pretrained('gpt2')\nmodel = GPT(GPTConfig())\nprint(\"gpt model loaded\")","metadata":{"_uuid":"d42fd6ac-f92e-40df-89fe-dbfa67840c26","_cell_guid":"949f73a3-5f74-4442-8cf2-0ade4ee90a0a","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-06-28T18:12:12.793530Z","iopub.execute_input":"2025-06-28T18:12:12.794250Z","iopub.status.idle":"2025-06-28T18:12:15.360631Z","shell.execute_reply.started":"2025-06-28T18:12:12.794223Z","shell.execute_reply":"2025-06-28T18:12:15.359983Z"}},"outputs":[{"name":"stdout","text":"gpt model loaded\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"num_return_sequences = 1\nmax_length = 30\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel.eval()\nmodel.to(device)","metadata":{"_uuid":"e4dfe941-aa55-4a42-a6ab-61eee9b8f2c7","_cell_guid":"10662572-8bed-4ee9-a564-f7b67f858f4f","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-06-28T18:12:15.361347Z","iopub.execute_input":"2025-06-28T18:12:15.361584Z","iopub.status.idle":"2025-06-28T18:12:15.693643Z","shell.execute_reply.started":"2025-06-28T18:12:15.361567Z","shell.execute_reply":"2025-06-28T18:12:15.692889Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"GPT(\n  (transformer): ModuleDict(\n    (wte): Embedding(50257, 768)\n    (wpe): Embedding(1024, 768)\n    (h): ModuleList(\n      (0-11): 12 x Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): CausalSelfAttention(\n          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): MLP(\n          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n          (gelu): GELU(approximate='tanh')\n          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n        )\n      )\n    )\n    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n)"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"import tiktoken\nenc = tiktoken.get_encoding('gpt2')\n\ntokens = enc.encode(\"Hello, I am a language model,\")\ntokens = torch.tensor(tokens, dtype=torch.long)\ntokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1)\nx = tokens.to(device)","metadata":{"_uuid":"902afa5e-6096-4043-86c1-767848eb5071","_cell_guid":"30df645b-c431-416d-8416-8ba0b8ba33a9","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-06-28T18:12:15.694411Z","iopub.execute_input":"2025-06-28T18:12:15.694664Z","iopub.status.idle":"2025-06-28T18:12:20.141398Z","shell.execute_reply.started":"2025-06-28T18:12:15.694644Z","shell.execute_reply":"2025-06-28T18:12:20.140604Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"print(f\"GPUs ({torch.cuda.device_count()}) available. Running inference...\")\ntorch.manual_seed(42)\ntorch.cuda.manual_seed(42)\ntemperature = 0.8\nwhile x.size(1) < max_length:\n    with torch.no_grad():\n        logits = model(x)\n        if isinstance(logits, tuple):\n            logits = logits[0]  # Unpack logits from tuple\n\n        logits = logits[:, -1, :]\n\n        probs = F.softmax(logits, dim=-1)\n        topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n        ix = torch.multinomial(topk_probs, 1)\n        xcol = torch.gather(topk_indices, -1, ix)\n        x = torch.cat((x, xcol), dim=1)\nprint(\"Inference finished.\")","metadata":{"_uuid":"990219f8-ad8f-4310-8515-6397825f64b0","_cell_guid":"f53120b9-c17e-4511-86bc-8f72377b1594","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-06-28T18:12:20.142150Z","iopub.execute_input":"2025-06-28T18:12:20.142345Z","iopub.status.idle":"2025-06-28T18:12:21.008119Z","shell.execute_reply.started":"2025-06-28T18:12:20.142330Z","shell.execute_reply":"2025-06-28T18:12:21.007271Z"}},"outputs":[{"name":"stdout","text":"GPUs (1) available. Running inference...\nInference finished.\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"for i in range(num_return_sequences):\n    tokens = x[i, :max_length].tolist()\n    decoded = enc.decode(tokens)\n    print(\"> \", decoded)","metadata":{"_uuid":"479286ab-9230-465f-bfc0-ea5918c9b441","_cell_guid":"3c5af5a1-bee4-45a4-93fb-6200ae166223","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-06-28T18:12:21.008980Z","iopub.execute_input":"2025-06-28T18:12:21.009803Z","iopub.status.idle":"2025-06-28T18:12:21.014612Z","shell.execute_reply.started":"2025-06-28T18:12:21.009761Z","shell.execute_reply":"2025-06-28T18:12:21.013775Z"}},"outputs":[{"name":"stdout","text":">  Hello, I am a language model, competitions competitions modern rad hanged signaled legislaturepriv Peshitative bury imb Beir legislaurus Beirnovaspeak similar colonization Facilities Rebels\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt","metadata":{"_uuid":"add08cce-bc9a-4729-861a-7b9ce0e633e0","_cell_guid":"3ca8ee5e-44ca-4aae-85bd-6eb70f7d92b8","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-06-28T18:12:21.015438Z","iopub.execute_input":"2025-06-28T18:12:21.015744Z","iopub.status.idle":"2025-06-28T18:12:21.972075Z","shell.execute_reply.started":"2025-06-28T18:12:21.015722Z","shell.execute_reply":"2025-06-28T18:12:21.971158Z"}},"outputs":[{"name":"stdout","text":"--2025-06-28 18:12:21--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 1115394 (1.1M) [text/plain]\nSaving to: ‘input.txt’\n\ninput.txt           100%[===================>]   1.06M  4.79MB/s    in 0.2s    \n\n2025-06-28 18:12:21 (4.79 MB/s) - ‘input.txt’ saved [1115394/1115394]\n\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"class DataLoader:\n    def  __init__(self, B, T):\n        self.B = B\n        self.T = T\n\n        with open('input.txt', 'r') as f:\n            text = f.read()\n\n        enc = tiktoken.get_encoding('gpt2')\n        tokens = enc.encode(text)\n        self.tokens = torch.tensor(tokens)\n        print(f\"loaded {len(self.tokens)} tokens\")\n        print(f\"1 epoch = {len(self.tokens) // (B * T)} batches\")\n\n        self.current = 0\n    def next_batch(self):\n        B = self.B\n        T = self.T\n        # Ensure that we have enough tokens for a full batch\n        end = min(self.current + B * T + 1, len(self.tokens))\n        buf = self.tokens[self.current:end]\n        buf = buf.to(device)\n\n        if len(buf) < B * T + 1:\n            # If not enough tokens for a full batch, reset and take from the beginning\n            self.current = 0\n            end = B * T + 1\n            buf = self.tokens[self.current:end]\n            buf = buf.to(device)\n\n\n        x = (buf[:-1]).view(B, T)\n        y = (buf[1:]).view(B, T)\n        self.current += B * T\n\n        if self.current + (B*T + 1) > len(self.tokens):\n            print(\"ROUNDING\")\n            self.current = 0\n\n        return x, y","metadata":{"_uuid":"0c059b9d-09f6-40e3-aa21-9336e5c8763b","_cell_guid":"eeb6524e-7098-49cb-89de-ef7c5f326a48","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-06-28T18:12:21.973320Z","iopub.execute_input":"2025-06-28T18:12:21.973619Z","iopub.status.idle":"2025-06-28T18:12:21.981308Z","shell.execute_reply.started":"2025-06-28T18:12:21.973586Z","shell.execute_reply":"2025-06-28T18:12:21.980525Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"total_batch_size = 2**19\nB = 4\nT = 1024\nassert total_batch_size % (B * T) == 0\ngrad_accum_step = total_batch_size // (B * T)\nprint(f\"total desiderd batch size: {total_batch_size}\")\nprint(f\"calcualted gradienet accumulation steps: {grad_accum_step}\")\n\ntrainloader = DataLoader(B=B, T=T)\nx, y = trainloader.next_batch()\nx = x.to(device)\ny = y.to(device)\nmodel = GPT(GPTConfig(vocab_size=50304))\nmodel.to(device)\ntorch.compile(model)\nlogits, loss = model(x, y)\nprint(loss)","metadata":{"_uuid":"dc1cef4d-4be3-40f8-b35c-07bce34719ab","_cell_guid":"bc0e30d0-1946-4b58-b8f7-27cad0ed4b80","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-06-28T18:12:21.982015Z","iopub.execute_input":"2025-06-28T18:12:21.982236Z","iopub.status.idle":"2025-06-28T18:12:29.297147Z","shell.execute_reply.started":"2025-06-28T18:12:21.982214Z","shell.execute_reply":"2025-06-28T18:12:29.296368Z"}},"outputs":[{"name":"stdout","text":"total desiderd batch size: 524288\ncalcualted gradienet accumulation steps: 128\nloaded 338025 tokens\n1 epoch = 82 batches\ntensor(10.9501, device='cuda:0', grad_fn=<NllLossBackward0>)\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"max_lr = 6e-4\nmin_lr = max_lr * 0.1\nwarmup_step = 10\nmax_steps = 50\n\ndef get_lr(it):\n    if it < warmup_step:\n        return max_lr * (it + 1) / warmup_step\n\n    if it > max_steps:\n        return min_lr\n\n    decay_ratio = (it - warmup_step) / (max_steps - warmup_step)\n    assert 0 <= decay_ratio <= 1\n    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n    return min_lr + coeff * (max_lr - min_lr)","metadata":{"_uuid":"3b8d44e0-3b6b-44a6-8702-e5df797ed5fc","_cell_guid":"837027f5-8503-4c80-a948-79564bab90eb","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-06-28T18:12:29.298030Z","iopub.execute_input":"2025-06-28T18:12:29.298411Z","iopub.status.idle":"2025-06-28T18:12:29.304011Z","shell.execute_reply.started":"2025-06-28T18:12:29.298390Z","shell.execute_reply":"2025-06-28T18:12:29.303194Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"#optimizer = torch.optim.Adam(model.parameters(), lr=3e-4, betas=(0.9, 0.95), eps=1e-8)\noptimizer = model.configure_optimizer(weight_decay=0.1, lr=6e-4, device=device)\nprint(\n    f\"{'Step':>6} | {'Loss':>8} | {'LR':>10} | {'Time (ms)':>9} | {'GradNorm':>10} | {'Speed':>15}\"\n)\nfor step in range(max_steps):\n    t0 = time.time()\n    optimizer.zero_grad()\n    accum = 0.0\n    for micro_steps in range(grad_accum_step):\n        x, y = trainloader.next_batch()\n        x, y = x.to(device), y.to(device)\n    \n        with torch.autocast(device_type=device, dtype=torch.bfloat16):\n            logits, loss = model(x, y)\n        loss = loss / grad_accum_step\n        accum += loss.detach()\n        loss.backward()\n    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n    lr = get_lr(step)\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = lr \n    \n    optimizer.step()\n    torch.cuda.synchronize()\n    t1 = time.time()\n    dt = (t1 - t0) * 1000\n    tokensxsec = (trainloader.B * trainloader.T * grad_accum_step) / (t1 - t0)\n    print(\n        f\"{step:6d} | {accum.item():8.4f} | {lr:10.2e} | {dt:9.1f} | {norm:10.4f} | {tokensxsec:15.2f} tokens/sec\"\n    )","metadata":{"_uuid":"55f0442a-5c41-4474-9e35-271a6dd7bacd","_cell_guid":"6c7b0b63-285c-4836-a454-86b6fa3a4be5","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-06-28T18:12:29.304889Z","iopub.execute_input":"2025-06-28T18:12:29.305149Z"}},"outputs":[{"name":"stdout","text":"Num decayed paramter tensor: 50, with 124,354,560 params\nNum non decayed paramter tensor: 98, with 121,344 params\nusing fused AdamW: True\n  Step |     Loss |         LR | Time (ms) |   GradNorm |           Speed\n","output_type":"stream"}],"execution_count":null}]}